{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVJU_tO-h0qd",
        "outputId": "d7b7e5a1-778b-42d5-a997-4ed4a75f6659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Collecting annoy\n",
            "  Downloading annoy-1.17.3.tar.gz (647 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.3-cp310-cp310-linux_x86_64.whl size=550737 sha256=5d2e2b79b9187cda42f3632000f1ee05f4c967150b3e7ecc00e4452f26ca701e\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/8a/da/f714bcf46c5efdcfcac0559e63370c21abe961c48e3992465a\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.17.3\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install annoy\n",
        "!pip install tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from annoy import AnnoyIndex\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n"
      ],
      "metadata": {
        "id": "ngRY0ZWeq9GN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "MODEL_NAME = \"intfloat/multilingual-e5-large\"\n",
        "MAX_LENGTH = 512\n",
        "N_TREES = 100\n",
        "ANN_FILE = '/content/drive/MyDrive/훈련전용/1207/mteann3.ann'\n",
        "EMBEDDINGS_FILE = '/content/drive/MyDrive/훈련전용/1207/mteann3.npy'\n"
      ],
      "metadata": {
        "id": "FKvAMQ5nq9J_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def average_pool(last_hidden_states, attention_mask):\n",
        "    # Apply a mask to the last hidden states\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).float()\n",
        "    sum_embeddings = torch.sum(last_hidden_states * input_mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    mean_embeddings = sum_embeddings / sum_mask\n",
        "    return mean_embeddings"
      ],
      "metadata": {
        "id": "NWZV_CTEyX_G"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MULTIEmbedding:\n",
        "    def __init__(self, model_name=\"intfloat/multilingual-e5-large\"):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer, self.model = self.load_model()\n",
        "        if self.model is None or self.tokenizer is None:\n",
        "            raise RuntimeError(\"Model loading failed.\")\n",
        "\n",
        "    def load_model(self):\n",
        "        try:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "            model = AutoModel.from_pretrained(self.model_name)\n",
        "            logging.info(\"MULTIE5 model loaded successfully.\")\n",
        "            return tokenizer, model\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error loading MULTIE5 model: {e}\")\n",
        "            return None, None\n",
        "\n",
        "    def get_embeddings(self, docs, batch_size=10):\n",
        "        self.model = self.model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        embeddings = []\n",
        "        logging.info(\"Starting embedding generation.\")\n",
        "\n",
        "        for i in tqdm(range(0, len(docs), batch_size), desc=\"Generating embeddings\", total=len(docs)//batch_size + 1):\n",
        "            batch = docs[i:i+batch_size]\n",
        "            inputs = self.tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LENGTH)\n",
        "            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "            batch_embeddings = average_pool(outputs.last_hidden_state, inputs['attention_mask'])\n",
        "            batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1)  # Normalize embeddings\n",
        "            embeddings.extend(batch_embeddings.cpu().detach().numpy())\n",
        "\n",
        "        logging.info(\"Embedding generation complete.\")\n",
        "        return np.array(embeddings)\n",
        "\n"
      ],
      "metadata": {
        "id": "3JHsWL_Jq9Od"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AnnoyIndexBuilder:\n",
        "    def __init__(self, embedding_dim, n_trees=N_TREES, ann_file=ANN_FILE):\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_trees = n_trees\n",
        "        self.ann_file = ann_file\n",
        "\n",
        "    def build_and_save(self, embeddings):\n",
        "        if embeddings is None or len(embeddings) == 0:\n",
        "            raise ValueError(\"No embeddings provided.\")\n",
        "\n",
        "        t = AnnoyIndex(self.embedding_dim, 'angular')\n",
        "        logging.info(\"Building Annoy index.\")\n",
        "\n",
        "        for i, vec in enumerate(tqdm(embeddings, desc=\"Building Annoy Index\")):\n",
        "            t.add_item(i, vec)\n",
        "\n",
        "        t.build(self.n_trees)\n",
        "        t.save(self.ann_file)\n",
        "        logging.info(\"Annoy index built and saved.\")\n",
        "        return t\n",
        "\n",
        "    def load(self):\n",
        "        u = AnnoyIndex(self.embedding_dim, 'angular')\n",
        "        if not u.load(self.ann_file):\n",
        "            raise IOError(f\"Could not load Annoy index from {self.ann_file}\")\n",
        "        logging.info(\"Annoy index loaded.\")\n",
        "        return u\n",
        "\n"
      ],
      "metadata": {
        "id": "Zfu9Qt5tq9SA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_embeddings(embeddings, filename=EMBEDDINGS_FILE):\n",
        "    try:\n",
        "        np.save(filename, embeddings)\n",
        "        logging.info(\"Embeddings saved.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error saving embeddings: {e}\")\n",
        "\n",
        "\n",
        "def load_embeddings(filename=EMBEDDINGS_FILE):\n",
        "    if os.path.exists(filename):\n",
        "        return np.load(filename)\n",
        "    else:\n",
        "        logging.error(f\"Embeddings file {filename} not found.\")\n",
        "        return None\n",
        "\n",
        "def query_index(query, embedding_model, annoy_index, top_n=5):\n",
        "    query_vec = embedding_model.get_embeddings([query])[0]\n",
        "    nns = annoy_index.get_nns_by_vector(query_vec, top_n)\n",
        "    return nns\n",
        "\n"
      ],
      "metadata": {
        "id": "up05VlNdq9W0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Initialize MULTIE5 model for embeddings\n",
        "    embedding_model = MULTIEmbedding()\n",
        "\n",
        "# Paths to your TXT files\n",
        "    txt_file_paths = [\"/content/drive/MyDrive/훈련전용/병원csv데이터/질환정리1212.txt\",\n",
        "                     \"/content/drive/MyDrive/훈련전용/병원csv데이터/아산병원데이터.txt\",\n",
        "                      \"/content/drive/MyDrive/훈련전용/병원csv데이터/대통합데이터.txt\"]\n",
        "\n",
        "\n",
        "\n",
        "    def read_text_file(file_path):\n",
        "        for encoding in ['utf-8', 'utf-16', 'ISO-8859-1']:\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding=encoding) as file:\n",
        "                    return [line.strip() for line in file]\n",
        "            except UnicodeDecodeError:\n",
        "                continue\n",
        "        raise ValueError(f\"Failed to open file {file_path} with common encodings.\")\n",
        "\n",
        "    docs = []\n",
        "    for txt_file_path in txt_file_paths:\n",
        "        docs.extend(read_text_file(txt_file_path))\n",
        "\n",
        "    # # Read and store each line in the files as a document\n",
        "    # docs = []\n",
        "    # for txt_file_path in txt_file_paths:\n",
        "    #     with open(txt_file_path, 'r', encoding='utf-8') as file:\n",
        "    #         for line in file:\n",
        "    #             docs.append(line.strip())  # Adds each line as a separate document\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Paths to your CSV files\n",
        "    # csv_file_paths = [\"/content/drive/MyDrive/훈련전용/병원csv데이터/대통합데이터.csv\",\n",
        "    #                   \"/content/drive/MyDrive/훈련전용/병원csv데이터/삼성서울병원_6컬럼.csv\",\n",
        "    #                   \"/content/drive/MyDrive/훈련전용/병원csv데이터/성모병원데이터.csv\",\n",
        "    #                   \"/content/drive/MyDrive/훈련전용/병원csv데이터/세브란스데이터.csv\",\n",
        "    #                   \"/content/drive/MyDrive/훈련전용/병원csv데이터/아산병원데이터.csv\",\n",
        "    #                   \"/content/drive/MyDrive/훈련전용/병원csv데이터/아산병원데이터en.csv\",]\n",
        "\n",
        "    # # Read and concatenate the specified columns for each document\n",
        "    # docs = []\n",
        "    # for csv_file_path in csv_file_paths:  # This line is changed\n",
        "    #     df = pd.read_csv(csv_file_path)\n",
        "    #     for index, row in df.iterrows():\n",
        "    #         # Combine the text from all relevant columns into a single document string\n",
        "    #         document = ' '.join(str(row[col]) if not pd.isnull(row[col]) else '' for col in [\"질병명\", \"진료과\", \"증상\", \"관련질환\", \"동의어\", \"부위\"])\n",
        "    #         docs.append(document)\n",
        "\n",
        "    # Generate or load embeddings\n",
        "    if os.path.exists(EMBEDDINGS_FILE):\n",
        "        embeddings = load_embeddings()\n",
        "    else:\n",
        "        embeddings = embedding_model.get_embeddings(docs)\n",
        "        save_embeddings(embeddings)\n",
        "\n",
        "        # Check if embeddings are saved successfully\n",
        "        if not os.path.exists(EMBEDDINGS_FILE):\n",
        "            raise FileNotFoundError(\"Failed to save embeddings file.\")\n",
        "\n",
        "    # Build or load Annoy index\n",
        "    annoy_builder = AnnoyIndexBuilder(embedding_dim=embeddings.shape[1])\n",
        "    if os.path.exists(ANN_FILE):\n",
        "        annoy_index = annoy_builder.load()\n",
        "    else:\n",
        "        annoy_index = annoy_builder.build_and_save(embeddings)\n",
        "\n",
        "    # Querying\n",
        "    query = input(\"Enter your query text: \")\n",
        "    top_n = int(input(\"Enter number of top results to fetch: \"))\n",
        "    nearest_neighbors = query_index(query, embedding_model, annoy_index, top_n)\n",
        "\n",
        "    for nn in nearest_neighbors:\n",
        "        print(f\"Document {nn+1}: {docs[nn]}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRHaU7Wkq9aU",
        "outputId": "8e1eaff1-645a-470e-ebd0-93598d0cc2c7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your query text: 보챔,발진,식욕부진,반점\n",
            "Enter number of top results to fetch: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00, 29.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 835: 발열, 빈맥, 저혈압, 심계항진, 발한\n",
            "Document 3785: 발열, 불안감, 불쾌감\n",
            "Document 2938: 돌발적인 두통, 구토, 반신불수\n",
            "Document 5729: 홍역(Measles),감염내과,\"열,기침,림프 부종,식욕부진,피부소양감,콧물,코플릭 반점\",\"디프테리아,백일해\",rubeola,전신\n",
            "Document 5895: 홍역(Measles),감염내과,\"코플릭 반점,림프 부종,기침,열,식욕부진,콧물,피부소양감\",\"백일해,디프테리아\",rubeola,피부\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kobert-transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDUPtQQx3XwV",
        "outputId": "ae0b1ad1-381d-41b0-8052-5a8a7bff45c0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kobert-transformers\n",
            "  Downloading kobert_transformers-0.5.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from kobert-transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: transformers<5,>=3 in /usr/local/lib/python3.10/dist-packages (from kobert-transformers) (4.35.2)\n",
            "Collecting sentencepiece>=0.1.91 (from kobert-transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->kobert-transformers) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->kobert-transformers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->kobert-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->kobert-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->kobert-transformers) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->kobert-transformers) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->kobert-transformers) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=3->kobert-transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=3->kobert-transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=3->kobert-transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=3->kobert-transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=3->kobert-transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=3->kobert-transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=3->kobert-transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=3->kobert-transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=3->kobert-transformers) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->kobert-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5,>=3->kobert-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5,>=3->kobert-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5,>=3->kobert-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5,>=3->kobert-transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.1.0->kobert-transformers) (1.3.0)\n",
            "Installing collected packages: sentencepiece, kobert-transformers\n",
            "Successfully installed kobert-transformers-0.5.1 sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "# KoBERT model checkpoint\n",
        "kobert_model_checkpoint = \"monologg/kobert\"\n",
        "\n",
        "# Initialize the tokenizer and model directly from the Hugging Face model repository\n",
        "tokenizer = BertTokenizer.from_pretrained(kobert_model_checkpoint)\n",
        "model = BertModel.from_pretrained(kobert_model_checkpoint)\n",
        "\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def get_embeddings(text, tokenizer, model):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Use mean pooling for embeddings\n",
        "    input_mask_expanded = inputs['attention_mask'].unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
        "    sum_embeddings = torch.sum(outputs.last_hidden_state * input_mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    mean_embeddings = sum_embeddings / sum_mask\n",
        "    return mean_embeddings.cpu().numpy()\n",
        "\n",
        "def compute_similarity(doc, query, tokenizer, model):\n",
        "    doc_embedding = get_embeddings([doc], tokenizer, model)\n",
        "    query_embedding = get_embeddings([query], tokenizer, model)\n",
        "    sim = cosine_similarity(doc_embedding, query_embedding)[0][0]\n",
        "    return sim\n",
        "\n",
        "# Example usage\n",
        "doc = \"요로감염,배뇨곤란,긴박뇨,빈뇨,지연뇨,배뇨장애,혈뇨,야간뇨,잔뇨감\"\n",
        "query = \"오줌이 곤란하다, 밤에 화장실을 가야한다\"\n",
        "similarity_score = compute_similarity(doc, query, tokenizer, model)\n",
        "print(f\"Similarity: {similarity_score}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3JkK8G7q9e0",
        "outputId": "aee1bf7a-a922-4362-8fdb-5040fbf89020"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity: 0.6967868208885193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iT13uMHmq9ic"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DzsRsFBGq9m0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TS7rMWToq9qc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uLX6cAbCq9uW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YdAAPwjqq9x_"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}